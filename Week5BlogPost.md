<span style= "font-size:16px"> **How can machine learning support people's existing creative practices? Expand people's creative capabilities?**</span>

Machine learning can support people’s existing creative practices by allowing them to learn things quicker. As mentioned in the video, machine learning can create faster prototyping and testing, which means that creators can figure out what works and what works best much faster, and allows for much more experimentation. Being able to try out more variations is profoundly helpful because it allows creators to explore much further in fields that they are already creating and creates many new possibilities that would probably be lost within time constraints. People’s creative abilities can be expanded because they can do things that they don’t know how to do in real life or things that would be impossible physically with machine learning. For example, in the video she talks about a product that changes beatboxing into sounds from musical instruments. This expands people’s creative capabilities because now someone who only knows how to beatbox is able to create music like someone that knows how to play an instrument. She also brings up things that are impossible to do in real life, like creating sounds on instruments that would be impossible to do with the same physical version of that instrument. This is also great for people with limited mobility to play instruments that they would not be able to physically play. 


<span style= "font-size:16px"> **Dream up and design the inputs and outputs of a real-time machine learning system for interaction and audio/visual performance. This could be an idea well beyond the scope of what you can do in a weekly exercise.
**</span>
One of the things that machine learning can do that is really powerful is making a learning system for audio and visual performance much more accessible. Just like the creatability project showed, you can create instruments that can require very little movement, allowing people that do not have a large range of movement to be able to interact and learn with them. My idea is that I would create something that can put together sound and visuals without having to move anything but your eyes. For this, we could use PoseNet because it does track where your eyes are. So, I would use the location of the eyes and their movement along the screen and draw a line as the eyes move, while also having the background be a synth. So, when the eyes are in a certain position, it will play a note and moving up or down can change the octave and moving to the side can change the note. For the visualization it could also be something like having particles that stay where the eyes are that move across the screen as well as there are many different possibilities for how to visualize this. So, with just eye movement, someone could create a visualization of a composition.


<span style= "font-size:16px"> **My p5 sketch**</span>

For my p5 sketch, I wanted to try what I did last week but this time be able to integrate machine learning so that shrugging and raising either hand would not have to be so exact. Also, hopefully I would be able to integrate adding multiple questions. However, I was not able to finish because I had trouble training the model. For some reason, it has not been allowing me to type in the console when I run the model, which means that I cannot train the model and I’m not sure where this problem is coming from. However, if I was able to train the model I believe that this would work much better than my model last week because I struggled to get that to work from different distances because of the exactness of the model. 

Here is my code anyways: [code](https://editor.p5js.org/aramakrishnan/sketches/BB-dWmjon)
