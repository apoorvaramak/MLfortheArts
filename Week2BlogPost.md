<span style= "font-size:16px"> **Reflect on the relationship between labels and images in a machine learning image classification dataset? Who has the power to label images and how do those labels and machine learning models trained on them impact society?** </span>
    
  Before reading this article things like ImageNet seemed like they had the potential to be used for less savory purposes, like facial recognition. 
  However, reading this article made me realize how insidious the categories themselves are. Labels on a training set are the only information that a 
  machine learning algorithm is given. So, to learn how to identify anything, it must rely on the initial images and labels. So, associating negative labels 
  with certain groups can be done very easily and widely spread by the computer without a second thought. The article talked about how workers were the ones that were 
  initially labelling images. Which makes sense because someone has to do it. However, this surprised me because I thought that the information came from other details 
  from where the image was from like the original caption, poster, and their profile. But all the labels just come from a person looking at a picture and deciding how 
  to label it. I knew that machine learning has the capability of being biased, but I never fully understood how until now. If someone is racist, sexist, or homophobic, 
  they can just impose their beliefs onto the machine learning algorithm by putting certain pictures of people under categories that are negative or offensive. This 
  impacts society as a whole because as society moves towards more automation, automation has to be able to keep up and live up to people’s expectations. People believe 
  that computers are unbiased, completely true sources of information, untouched by the prejudices of man. So, if a computer were to say that someone committed a crime, 
  it’s much more widely believed than an eyewitness account. The article mentioned another project, one that’s supposed to identify violence. If the training set is 
  theatrical and unlike real life, it could lead to actual consequences for people by having false positives and negatives. Overall, this is perfectly summed up by the 
  painting by Magritte. Images are just images, they are not people. Not everything about a person can be ascertained from an image of them. Acting like one or even three
  people identifying the person in the image can be completely accurate is woefully wrong and harmful for people, especially people who are discriminated against. 
  
  <span style= "font-size:16px"> **My P5 Project** </span>
  
  I was interested in the sound identification because I was wondering how the sound was classified. Would it be by the notes, the pitch, or something completely different all together? The example that was used was clapping but I wondered if specific sounds could be identified from each other. So, I tried something that is the same type of sound but in different variations, whistling. Waltz No. 2 by Shostakovich and Serenade for Strings in E major by Mozart were the two songs that I decided to try whistling. Since the sound clips are so short, I decided to choose iconic two-second segments from each and whistled them for the training set. Once I trained the set, I was surprised to see how well the classification worked. Most of the time, when I whistled each part the algorithm was accurate. However, there were a few pit falls. For the background noise I chose the white noise from my room, so when someone is talking normally, the microphone pics it up and it is still assigned either Serenade for Strings or Waltz No. 2 even though the person is not whistling either. Also, I was curious to see if it could identify the song if I played an instrumental version, but it was not able to at all. I'm not sure if that's because of how the identification works or if my whistling just was not accurate enough. Overall, I was impressed that it was able to tell the difference between my two different whistles, but there is still a lot more work to be done to make this much more accurate. 
  
